{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shLOkktCOKLu",
        "outputId": "faf82da7-1d05-43e8-e85e-86dc7ce9336d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\",force_remount=True)\n",
        "dataset=5 # 1-4:  whuGait,   5:  OU-ISIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hJmozS0VHbh"
      },
      "outputs": [],
      "source": [
        "data_path ='/content/drive/MyDrive/imuz/AutomaticExtractionData_IMUZCenter.rar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VcKwR3K2dFX",
        "outputId": "2d7d7374-fc7e-4ed7-aa55-e79eb10362be"
      },
      "outputs": [],
      "source": [
        "\n",
        "!apt-get install -y unrar\n",
        "import os\n",
        "rar_path = '/content/drive/MyDrive/imuz/AutomaticExtractionData_IMUZCenter.rar'\n",
        "extract_path = '/content/drive/MyDrive/imuz/autoz/'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "!unrar x \"{rar_path}\" \"{extract_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7w6Gfio4RmM"
      },
      "outputs": [],
      "source": [
        "data_path ='/content/drive/MyDrive/imuz/autoz/AutomaticExtractionData_IMUZCenter/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20FNeQsvg5R",
        "outputId": "169a5fd7-904a-4cd0-8c91-74d4482eae55"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import re\n",
        "# Define the directory where your CSV files are located\n",
        "csv_directory = data_path\n",
        "mini=1000\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        with open(os.path.join(csv_directory, filename), \"r\") as csv_file:\n",
        "            csv_reader = csv.reader(csv_file)\n",
        "            # Skip the first two rows\n",
        "            row=next(csv_reader)\n",
        "            f=row[0][6:10]\n",
        "            fint=int(f)\n",
        "            if(fint<mini):\n",
        "              mini=fint\n",
        "print(mini)\n",
        "K=int(mini/2)\n",
        "print(K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSisvT_WO4dd"
      },
      "outputs": [],
      "source": [
        "K=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YADF-r1y153O",
        "outputId": "89f3f583-2f92-445c-c3ab-0b6259ecc3a0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from csv import reader\n",
        "\n",
        "csv_directory ='/content/drive/MyDrive/imuz/autoz/AutomaticExtractionData_IMUZCenter/'\n",
        "array_list = []\n",
        "array_list_ts = []\n",
        "label = []\n",
        "labelts = []\n",
        "\n",
        "k = 128  \n",
        "\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        person_id = filename.split(\"_\")[1]  \n",
        "\n",
        "        with open(os.path.join(csv_directory, filename), 'r') as csv_file:\n",
        "            csv_reader = reader(csv_file)\n",
        "            next(csv_reader) \n",
        "            data = [list(map(float, row[:6])) for row in csv_reader if len(row) >= 6]\n",
        "\n",
        "        numlines = len(data)\n",
        "        num_samples = numlines // k\n",
        "\n",
        "        if num_samples < 5:\n",
        "            splitsi = 1\n",
        "        elif num_samples < 11:\n",
        "            splitsi = 2\n",
        "        elif num_samples < 16:\n",
        "            splitsi = 3\n",
        "        else:\n",
        "            splitsi = 4\n",
        "\n",
        "\n",
        "        segments = [data[i*k:(i+1)*k] for i in range(num_samples)]\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            segment_np = np.array(segment)\n",
        "            if i < splitsi:\n",
        "                array_list_ts.append(segment_np)\n",
        "                labelts.append(person_id)\n",
        "            else:\n",
        "                array_list.append(segment_np)\n",
        "                label.append(person_id)\n",
        "\n",
        "print(f\"âœ… number of training data  {len(array_list)}, number of testing data {len(array_list_ts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnuQOJ554y5R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling1D, Reshape, Dense, Multiply\n",
        "\n",
        "def squeeze_excite_block(input_tensor):\n",
        "    filters = input_tensor.shape[-1]\n",
        "    se = GlobalAveragePooling1D()(input_tensor)\n",
        "    se = Reshape((1, filters))(se)\n",
        "    se = Dense(filters // 8, activation='relu')(se)\n",
        "    se = Dense(filters, activation='sigmoid')(se)\n",
        "    return Multiply()([input_tensor, se])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlkR_xA95UYa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, AveragePooling1D\n",
        "\n",
        "def build_cnn_branch(input_tensor):\n",
        "    x = Conv1D(24, 1, padding='same')(input_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling1D(pool_size=2)(x)\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    x = Conv1D(40, 1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling1D(pool_size=2)(x)\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    x = Conv1D(104, 1, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling1D(pool_size=2)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyqLN81K5df4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Masking, LSTM, Dropout\n",
        "\n",
        "def build_lstm_branch(input_tensor):\n",
        "    x = Masking()(input_tensor)\n",
        "    x = LSTM(24, return_sequences=False)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYC9Vr_n5oM_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Concatenate, Dense, Reshape, Input, Flatten\n",
        "\n",
        "input_layer = Input(shape=(128, 6))\n",
        "cnn_output = build_cnn_branch(input_layer)\n",
        "cnn_output = Flatten()(cnn_output)\n",
        "lstm_output = build_lstm_branch(input_layer)\n",
        "\n",
        "merged = Concatenate()([cnn_output, lstm_output])\n",
        "final_output = Dense(num_classes, activation='softmax')(merged)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=final_output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z1XZx-B_Wjy"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(array_list)\n",
        "X_test = np.array(array_list_ts)\n",
        "y_train = np.array(label)\n",
        "y_test = np.array(labelts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m19x3D5g_o3H"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y_train_encoded = encoder.fit_transform(y_train)\n",
        "y_test_encoded = encoder.transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V9RpKSi_v3J"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "num_classes = len(np.unique(y_train_encoded))\n",
        "y_train_cat = to_categorical(y_train_encoded, num_classes)\n",
        "y_test_cat = to_categorical(y_test_encoded, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PA8hfdYBLK-",
        "outputId": "12e55da3-56d4-4674-c9b4-9adb08a58a5f"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train_cat,\n",
        "    validation_data=(X_test, y_test_cat),\n",
        "    epochs=50,\n",
        "    batch_size=16\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Ru5PwT88D6BA",
        "outputId": "6e0b2d02-eb05-4e6c-8dcb-7da9293c71e5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Model Training Accuracy')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "JnFa8IELRtAc",
        "outputId": "1de59d03-7555-476e-8451-7a75d64e2012"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import re\n",
        "# Define the directory where your CSV files are located\n",
        "csv_directory = data_path\n",
        "# Initialize an array to store person IDs and arrays to store file data\n",
        "person_ids = []\n",
        "file_data = []\n",
        "# Initialize an array to store the data from the current file\n",
        "current_file_data = []\n",
        "# Iterate through the files in the directory\n",
        "rows = 1\n",
        "columns = 6\n",
        "# Create an empty 2D array\n",
        "empty_2d_array = np.empty((rows, columns))\n",
        "array_list = []\n",
        "label=[]\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        # Extract person ID from the filename\n",
        "        person_id = filename.split(\"_\")[1]\n",
        "        # Store the person ID in the person_ids array\n",
        "        person_ids.append(person_id)\n",
        "        # Open and read the CSV file\n",
        "        with open(os.path.join(csv_directory, filename), \"r\") as csv_file:\n",
        "            csv_reader = csv.reader(csv_file)\n",
        "            # Skip the first two rows\n",
        "            next(csv_reader)\n",
        "            next(csv_reader)\n",
        "            # Iterate through the remaining rows\n",
        "            id=1\n",
        "            row_array=[]\n",
        "            empty_2d_array = np.empty((rows, columns))\n",
        "            for row in csv_reader:\n",
        "              # Convert each string to a NumPy array and append to the 'arrays' list\n",
        "              arrays = [np.fromstring(string1, sep=\",\") for string1 in row]\n",
        "              # Vertically stack the arrays to create a 2D NumPy array\n",
        "              try:\n",
        "                output_array = np.concatenate(arrays)\n",
        "                empty_2d_array = np.vstack((empty_2d_array, output_array))\n",
        "                id=id+1\n",
        "              except:\n",
        "                  id=1\n",
        "                  empty_2d_array = np.empty((rows, columns))\n",
        "              if(id%K==0):\n",
        "                  array_list.append(empty_2d_array)\n",
        "                  numbers_only = ''.join(re.findall(r'\\d+', person_id))\n",
        "                  label.append(int(numbers_only))\n",
        "                  id=1\n",
        "                  empty_2d_array = np.empty((rows, columns))\n",
        "                # Add the person ID as a new element in the numpy array\n",
        "                # Append the numpy array to the current_file_data array\n",
        "first_shape = array_list[0].shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiu9hAnhRvtK"
      },
      "outputs": [],
      "source": [
        "final = np.array(array_list)\n",
        "print(final.shape)\n",
        "print(len(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbHyUxHFk7aa"
      },
      "outputs": [],
      "source": [
        "X_train=final\n",
        "# Find unique values and their numerical indices\n",
        "unique_values, indices = np.unique(label, return_inverse=True)\n",
        "# Replace the original values with their numerical indices\n",
        "numerical_indices = indices.reshape(-1, 1)\n",
        "# Create a NumPy array from the numerical indices\n",
        "numerical_array = np.array(numerical_indices)\n",
        "train_label=numerical_array\n",
        "# Determine the number of bits required for binary representation\n",
        "numerical_array = train_label\n",
        "# Determine the maximum value in numerical_array\n",
        "max_value = numerical_array.max()\n",
        "\n",
        "# Initialize a new array filled with zeros\n",
        "binary_representation = np.zeros((numerical_array.shape[0], max_value + 1), dtype=np.uint8)\n",
        "\n",
        "# Use advanced indexing to set the corresponding elements to \"1\"\n",
        "binary_representation[np.arange(numerical_array.shape[0]), numerical_array.ravel()] = 1\n",
        "\n",
        "# Print the binary_representation\n",
        "print(binary_representation.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWeCMjiIE8eY"
      },
      "outputs": [],
      "source": [
        "train_label=binary_representation\n",
        "X_train=final\n",
        "print(train_label.shape)\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEpccZAobjsy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X_train, train_label, X_test, and test_label are already loaded and preprocessed\n",
        "\n",
        "# Define the CNN + LSTM model\n",
        "def create_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    # CNN layers\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    # LSTM layer\n",
        "    #model.add(LSTM(32, return_sequences=True))\n",
        "    model.add(LSTM(512, return_sequences=False))\n",
        "    # Dense layer\n",
        "    model.add(Dropout(0.5))\n",
        "    #model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # Output layer\n",
        "    model.add(Dense(num_classes, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Set the input shape and number of classes\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "num_classes = train_label.shape[1]\n",
        "# Create the model\n",
        "model = create_model(input_shape, num_classes)\n",
        "# Compile the model\n",
        "model.compile(loss='CategoricalCrossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "epochs = 300\n",
        "batch_size = 32\n",
        "X_train1, X_val, train_label1, val_label = train_test_split(X_train, train_label, test_size=0.15, random_state=42)\n",
        "history = model.fit(X_train1, train_label1, epochs=epochs, batch_size=batch_size, validation_data=(X_val, val_label))\n",
        "\n",
        "\n",
        "# Plot the accuracy and loss curves during training\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy / Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy/Loss')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AkyxLvip8-T"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot the accuracy and loss curves during training\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy / Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy/Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_nkhw_8J3-8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
